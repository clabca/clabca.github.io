{"cells":[{"cell_type":"markdown","metadata":{"id":"-GI4hIWA6sYR"},"source":["# **Análisis exploratorio y estadística**\n","# AE-16 - Introducción a la Regresión Lineal\n","\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1Igtn9UXg6NGeRWsqh4hefQUjV0hmzlBv\" width=\"100\" align=\"left\" title=\"Runa-perth\">\n","<br clear=\"left\">\n","Contenido opcional\n"]},{"cell_type":"markdown","source":["## <font color='blue'>__Regresión lineal simple__</font>\n","\n","El contenido de esta clase corresponde a la implementación utilizando Python + Numpy para generar una regresión lineal simple, y una regresión lineal múltiple. La idea es poder entender y traspasar el proceso de una notación matemática, a operaciones de código utilizando numpy. Finalmente, enseñar el uso de librerías y comprobar que nuestras implementaciones *from scratch*, obtienen resultados similares a las implementaciones estándar de las librerías.\n","\n","En primer lugar, importamos las librerías:\n"],"metadata":{"id":"6nVCxxaRHXdk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpIFR2mW6sYR"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n"]},{"cell_type":"markdown","metadata":{"id":"R0hHxHE_6sYS"},"source":["Para ilustrar el cómo podemos ajustar una regresión lineal simple, vamos a simular datos aleatorios. Vamos a generar una variable independiente `x`, con valores del 1 al 10, y nuestra variable dependiente `y` corresponderá a una combinación de números enteros obtenidos de forma aleatoria entre $[10,20]$, con nuestra variable $x$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhFh8N3s6sYS"},"outputs":[],"source":["np.random.seed(42) #Fijamos la semilla aleatoria\n","\n","x = np.arange(1,11) # Generamos 10 numeros desde el 1 al 10\n","# Generamos 10 numeros aleatorios entre 10 y 20,\n","# luego lo multiplicamos con nuestra variable x\n","y = np.random.randint(10, 20, size=10) * x\n","\n","\n","plt.scatter(x,y, color='r') # Grafiquemos como se ven nuestros datos.\n","plt.xlabel(\"Var $x$\")\n","plt.ylabel(\"Var $y$\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"r0GSCFOd6sYS"},"source":["La idea de la regresión lineal es poder modelar la dependencia de las variables independientes, con las variables dependientes. En este caso, nosotros explícitamente hemos generado esta dependencia de $y$ en función de $x$, pero en set de datos de la vida real, esta dependencia puede ser mucho más complicada de modelar. La forma de una regresión lineal simple corresponde a la siguiente ecuación:\n","\n","$$Y = \\beta_1 X + \\beta_0 + \\epsilon$$\n","\n","donde $Y$ corresponde a nuestra variable dependiente, $X$ a nuestra variable independiente, $\\beta_1$ al coeficiente de regresión el cual modela la relación de la variable $X$ con la variable $Y$, $\\beta_0$ corresponde al intercepto, y $\\epsilon$ corresponde a una variable que incluye un conjunto grande de factores, cada uno de los cuales influye en la respuesta sólo en\n","pequeña magnitud, a la que llamaremos error.\n","\n","En este caso, nosotros podemos aproximar los valores de $Y$, y los parámetros del modelo ($\\beta_1$ y $\\beta_0$). Por lo mismo, utilizaremos la notación $\\hat{Y}, \\hat{\\beta_1}, \\hat{\\beta_0}$ para referirnos a estas aproximaciones. Utilizando el **método de los mínimos cuadrados** podemos estimar los valores de los distintos parámetros:\n","\n","\n","- Primer paso, calcular $\\hat{\\beta_1}$\n","\n","$$\\hat{\\beta_1} = \\frac{(\\sum x \\sum y) - (n\\sum xy)}{(\\sum x)^2 - n\\sum x^2} =\\frac{\\sum(x - \\bar{x})(y - \\bar{y})}{\\sum(x - \\bar{x})^2}$$\n","\n","- Segundo paso, calcular $\\hat{\\beta_0}$ en función de $\\hat{\\beta_1}$\n","\n","$$\\hat{\\beta_0} = \\frac{\\sum y - \\hat{\\beta_1}\\sum x}{n} = \\bar{y} - \\hat{\\beta_1}\\bar{x}$$\n","\n","Vamos a utilizar la segunda parte de las fórmulas, utilizando una combinación de las funciones de suma y de obtener el promedio (`np.sum` y `np.mean`). En este caso vamos a transformar las sumatorias ($\\Sigma$) en `np.sum`, los promedios ($\\bar{x}$) en `np.mean`, y $n$ en `len(x)`. Cabe destacar que realizar las operaciones del tipo: $x - \\bar{x}$, se pueden realizar debido a que $x$ es un vector de numpy, y este permite operaciones entre vectores y escalares.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0FPS-nXZ6sYS"},"outputs":[],"source":["x_min_mx = x - np.mean(x)  # x minus mean(x)\n","y_min_my = y - np.mean(y)  # y minus mean(y)\n","\n","beta_1 = np.sum(x_min_mx * y_min_my)  / np.sum(x_min_mx**2)\n","beta_0 = np.mean(y) - (beta_1 * np.mean(x))\n","\n","print(f'beta_1: {beta_1:5.2f} \\nbeta_0: {beta_0:5.2f}')"]},{"cell_type":"markdown","metadata":{"id":"b_3QSL5M6sYS"},"source":["### <font color='green'>**Actividad 1**</font>\n","\n","Implementar la primera forma de cada uno de los parámetros:\n","\n","$$ \\hat{\\beta_1} = \\frac{(\\sum x \\sum y) - (n\\sum xy)}{(\\sum x)^2 - n\\sum x^2}$$\n","y\n","$$ \\hat{\\beta_0} = \\frac{\\sum y - \\hat{\\beta_1}\\sum x}{n}$$\n","\n","- Compruebe que los valores de obtenidos con la primera forma y la segunda forma son iguales\n","- ¿Qué pasaría con nuestros coeficientes si cambiamos la forma de generar nuestra variable dependiente $Y$? Fije una semilla y reemplace el operador utilizado (en vez de multiplicar por $x$, cambiar por ej: suma)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIR3cE3u6sYS"},"outputs":[],"source":["# Tu código aquí ...\n"]},{"cell_type":"markdown","metadata":{"id":"TPe7B1L_6sYS"},"source":["<font color='green'>Fin Actividad 1</font>"]},{"cell_type":"markdown","metadata":{"id":"TeWx0pKB6sYS"},"source":["Con los parámetros $\\hat{\\beta_1}$ y $\\hat{\\beta_0}$, podemos generar una función la cual reciba el valor el vector $x$, y realizar la estimación de $\\hat{Y}$:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U8QyYXZD6sYS"},"outputs":[],"source":["# Llamaremos a esta función, lin_reg(x), recibe el vector y utiliza las variables betas calculadas anteriormente\n","# Una posible mejora, es que reciba los betas por parámetros de la función\n","def lin_reg(x):\n","    y_hat = (beta_1 * x) + beta_0 #Aplicamos la forma de la regresión lineal\n","    return y_hat"]},{"cell_type":"markdown","metadata":{"id":"N7tYyXQY6sYS"},"source":["Con esta función definida, podemos graficar nuestra recta obtenida utilizando la función `lin_reg`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iC53ongH6sYS"},"outputs":[],"source":["plt.scatter(x,y,label=\"Datos originales (Ground Truth)\", color='r') # Vamos a gráficar nuestros datos originales\n","plt.plot(x,lin_reg(x), color=\"black\", label=\"Regresión lineal\") #Luego graficamos la curva ajustada\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"41xjIrmJ6sYS"},"source":["Podemos ver que nuestra regresión es bastante cercana a los valores reales utilizados para ajustar la regresión. Nosotros podemos calcular la bondad de ajuste de nuestra regresión, mediante el análisis de los errores. El error se define como: $y_i - \\hat{y_i}$ y corresponde a la resta del valor real $i$-ésimo, con el valor ajustado $i$-ésimo.\n","\n","Utilizando Numpy, este cálculo es relativamente sencillo, para mostrar el paso a paso, vamos a tener los valores del vector $y$:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9Q9ZR3m6sYS"},"outputs":[],"source":["print(y)"]},{"cell_type":"markdown","metadata":{"id":"64GNaSwL6sYS"},"source":["Por otra parte, vamos a tener los valores del vector $\\hat{y}$:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qs09G6Rn6sYS"},"outputs":[],"source":["print(lin_reg(x))"]},{"cell_type":"markdown","metadata":{"id":"Dvb9nL_L6sYS"},"source":["Luego, los errores $e$, se obtienen simplemente con la resta entre estos dos elementos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bbTUTKl86sYS"},"outputs":[],"source":["e = y - lin_reg(x)\n","print(e)"]},{"cell_type":"markdown","metadata":{"id":"V0TSnL0p6sYS"},"source":["Estos errores, podemos graficarlos utilizando el método de matplotlib `vlines` (Vertical Lines). Este método recibe como mínimo 3 parámetros: `x` (los valores en el eje X), `y_min` el inicio de la línea vertical, `y_max` el final de la línea vertical. En este caso, `y_min` va a corresponder a los valores reales $y$, e `y_max` corresponderán a los valores ajustados $\\hat{y}$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"id4iFVoh6sYS"},"outputs":[],"source":["fig, ax = plt.subplots(1,1)\n","ax.plot(x,lin_reg(x), color=\"black\", label=\"Regresión lineal\")\n","ax.scatter(x,y, label=\"Datos originales(Ground Truth)\", color='r', zorder=3)\n","\n","ax.vlines(x, y, lin_reg(x), color=\"b\", label=\"Error (Residuos)\") #Gráficamos los Errores en cada punto.\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JZlNvOaB6sYS"},"source":["Como anteriormente lo dijimos, la regresión se ajusta bastante bien a los datos simulados. Por lo mismo, ahora vamos a cuantificar qué tan bien se ajusta la regresión. Para esto, vamos a calcular el $R^2$, en función de la __Suma Total de los Cuadrados__, $SST$) y la __Suma de los Errores Cuadrados__, ($SSE$).\n","\n","El $SST$ obtiene su nombre de su sigla en inglés **Sum of Squares Total**, y esta mide la variabilidad total dentro del set de datos (específicamente dentro de nuestra variable dependiente). Esta se calcula como:\n","\n","$$ SST = \\sum_i (y_i - \\bar{y})^2$$\n","\n","Por otra parte, el $SSE$ obtiene su nombre de su sigla en inglés **Sum of Squares Error**, y mide la diferencia total entre los valores reales (u observados) y los valores ajustados (o predichos). Esta se calcula como:\n","\n","$$ SSE = \\sum_i (y_i - \\hat{y})^2 = \\sum_i e_i^2$$\n","\n","Con estas dos métricas podemos calcular el __coeficiente de determinación__, $R^2$. Esto nos proporcionara una métrica que podemos utilizar para cuantificar cuanta variabilidad explica nuestro modelo de regresión. Esta se calcula de la siguiente forma utilizando las dos métricas mencionadas anteriormente.\n","\n","$$ R^2 = 1 - \\frac{SSE}{SST}$$\n","\n","El $R^2$ puede resultar en valores entre $[0,1]$, donde si $R^2 = 1$, significa que la variable utilizada para la regresión lineal simple explica el 100% del comportamiento de la variable dependiente. Por el contrario, un $R^2 = 0$ significa que la variable independiente utilizada, no es capaz de explicar el comportamiento de la variable dependiente.\n","\n","__Nota__: Un coeficiente de determinación $R^2$ negativo en una regresión lineal es generalmente una señal de que el modelo no se ajusta bien a los datos y que incluso se desempeña peor que un modelo horizontal simple que predice la media de la variable dependiente para todas las observaciones.\n","\n","Calculemos el $R^2$ para nuestros datos sintéticos:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DRz251tn6sYS"},"outputs":[],"source":["SST = np.sum((y - np.mean(y))**2) #Calculamos la suma total de los cuadrados\n","SSE = np.sum((y - lin_reg(x))**2) # Calculamos la suma de los errores cuadrados\n","R2 = 1 - (SSE / SST) # Computamos el R^2\n","\n","print(f\"El R^2 obtenido para nuestros datos sintéticos es de {R2:6.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"IDX6CuTm6sYS"},"source":["Con esto, podemos decir que nuestra regresión lineal simple, es capaz de explicar un $92.51\\%$ de la variabilidad del comportamiento de nuestra variable dependiente sintética. Esto hace bastante sentido, ya que ese $7.49\\%$ que no es capaz de explicar, principalmente se debe a la aleatoriedad que nosotros introdujimos en la variable dependiente (`np.random.randint(10, 20, size=10)`)."]},{"cell_type":"markdown","metadata":{"id":"ArY8yYWe6sYS"},"source":["Otras métricas existentes para poder ver que tan buena es nuestra regresión, consisten en el análisis de los errores. Si nuestra regresión fuese perfecta, podríamos ver que el  $SSE = 0$, indicando claramente que nuestros valores predichos, son iguales a los valores observados.\n","\n","Estas métricas incluyen el __Error Cuadrático Medio__ (__Mean Squared error__, $MSE$), El __Error Absoluto Medio__ (__Mean Absolute Error__, $MAE$) y el __Error Absoluto Porcentual Promedio__ (__Mean Absolute Percentage Error__, $MAPE$). Idealmente, si utilizamos nuestra regresión para poder hacer pronósticos, nosotros queremos que los errores que podamos cometer sean mínimos (de lo contrario, sería una mala predicción). Estas métricas nos permiten evaluar el error de nuestros modelos, y por consiguiente decir si nuestra regresión podría utilizarse para realizar pronósticos.\n","\n","Interpretaciónes que se les pueden dar a estas métricas son bastante directas: el $MAE$ nos dirá una magnitud promedio de los residuos, dando una vista general de como podría comportarse nuestra regresión. El $MAPE$ nos indicara porcentualmente, que tanto se aleja nuestros valores predichos, de los valores reales. Finalmente, el $MSE$ cumple una misma función que el $MAE$, pero como estamos usando el cuadrado de los errores, esta métrica nos indicara que tan bueno es nuestro modelo, con respecto a valores atípicos (el error va a aumentar considerablemente para puntos alejados de nuestra regresión). Esto lo podrían visualizar en el gráfico de los errores visto anteriormente.\n","\n","Las fórmulas para cada una de estas métricas son:\n","\n","$$ MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y})^2$$\n","\n","$$ MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}|$$\n","\n","$$ MAPE = \\frac{1}{n} \\sum_{i=1}^n |\\frac{y_i - \\hat{y}}{y_i}|$$\n","\n","**Nota:**, MAPE se indefine para valores de $y_i = 0$, por lo mismo, no es una métrica comúnmente utilizada.\n","\n","\n","Utilizando `numpy` podemos computarlas:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEtf4NSr6sYS"},"outputs":[],"source":["mse = np.sum((y - lin_reg(x))**2) / len(y)\n","mae = np.sum(np.abs(y - lin_reg(x))) / len(y)\n","mape = np.sum(np.abs((y - lin_reg(x)) / y)) / len(y)"]},{"cell_type":"markdown","metadata":{"id":"zZsCeSIk6sYS"},"source":["Luego, simplemente las imprimimos y vemos que si bien, el $MAE$ y el $MAPE$ son bajos, los puntos {6, 7, 9, 10} de los datos, influyen considerablemente en el $MSE$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJMTg73X6sYS"},"outputs":[],"source":["print(f'MSE = {mse:6.2f}\\nMAE = {mae:6.2f}\\nMAPE = {mape:5.2f}')"]},{"cell_type":"markdown","metadata":{"id":"1FLU-77K6sYS"},"source":["Alternativamente, podemos utilizar la librería de `scikit-learn` para poder usar sus implementaciones del $MSE$, $MAE$ y el $R^2$ (el $MAPE$ no está implementado en esta librería). Cada uno de estos métodos, recibe dos parámetros: los valores observados/reales `y_real` y los valores predichos/ajustados `y_pred`.\n","\n","Luego el uso de estas funciones son tan simples como por ejemplo:\n","```python\n","mean_squared_error(y_real , y_pred)\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d4iStwE86sYT"},"outputs":[],"source":["# importamos las librerias\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"50lqMBTT6sYT"},"outputs":[],"source":["# calculamos cada una de estas métricas, y podemos ver que los valores\n","# son iguales a nuestras implementaciones en Numpy puro.\n","mse = mean_squared_error(y , lin_reg(x))\n","mae = mean_absolute_error(y , lin_reg(x))\n","r2= r2_score(y , lin_reg(x))\n","\n","print(f'MSE      = {mse:6.2f}\\nMAE      = {mae:6.2f}\\nR2 Score = {r2:6.2f}')"]},{"cell_type":"markdown","metadata":{"id":"p_W3hRmI6sYT"},"source":["### <font color='green'>**Actividad 2**:</font>\n","### Predicción del consumo de cerveza\n","\n","Este set de datos fue recolectado en São Paulo - Brasil. Específicamente en un área universitaria con grupos de estudiantes entre 18 a 28 años de edad. Existen 7 variables en este set de datos:\n","\n","- __Data__: la fecha en que se tomó la medición\n","- __Temperatura Media ($^oC$)__: la temperatura promedio durante todo un día\n","- __Temperatura Mínima ($^oC$)__: la temperatura mínima durante todo un día\n","- __Temperatura Máxima ($^oC$)__: la temperatura máxima durante todo un día\n","- __Precipitacao (mm)__: cantidad de mm de precipitaciones (lluvia):\n","- __Final de Semana__: Si era fin de semana (o no).\n","- __Consumo de cerveja (litros)__: La cantidad de cervezas consumidas en promedio de los grupos.\n","\n","Esta última variable, corresponde a nuestra variable dependiente. La dinámica consiste en encontrar la variable independiente que explique mejor el consumo de cerveza. Ante esto, deberán ajustar $5$ modelos de regresión lineal simple y obtener las métricas de bondad de ajuste y de errores para cada uno de estos modelos. Indique cuál fue esta variable e imprima por pantalla cuales fueron los valores de $R^2$ y $MAE$, $MAPE$ y $MSE$. Finalmente, haga el gráfico de la regresión lineal mostrando los errores obtenidos por la variable más explicativa.\n","\n","Archivo: `Consumo_cerveja.csv`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nriz9g64rmzG"},"outputs":[],"source":["# Montar Drive si corresponde\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pje1n3sc6sYT"},"outputs":[],"source":["# Tu código aquí ... ...\n","\n","import pandas as pd\n","import numpy as np\n","cerveza_df = pd.read_csv(\"../Consumo_cerveja.csv\")\n","cerveza_df.head().T\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-kbDJznT6sYT"},"source":["<font color='green'>Fin Actividad 2</font>"]},{"cell_type":"markdown","metadata":{"id":"BIvM5tX-6sYT"},"source":["## <font color='blue'>**Regresión lineal con múltiples variables independientes**</font>\n","\n","\n","Utilizando el mismo dataset obtenido de la dinámica 2, vamos a ajustar una regresión lineal múltiple. Esto quiere decir, que, en vez de ajustar un solo parámetro en nuestra regresión para tratar de explicar la variable dependiente, utilizaremos dos o más variables dependientes. En este caso, nuestra regresión tomara una nueva forma:\n","\n","\n","$$Y =  \\beta_0 +X_1\\beta_1+X_2\\beta_2+\\dots+X_m\\beta_m$$\n","\n","En este caso vamos a tener $m+1$ parámetros, donde $m$ corresponde a la cantidad de variables dependientes que utilizaremos. La razón de que sean $m+1$ variables, es porque tenemos que considerar el intercepto $\\hat{\\beta_0}$.\n","\n","La ecuación anterior, podemos representarlas en forma matricial:\n","\n","$$ Y = X^T\\beta$$\n","\n","donde:\n","\n","$$Y = \\begin{bmatrix}Y_1 \\\\ Y_2  \\\\ \\vdots \\\\ Y_m\\end{bmatrix} \\quad \\beta = \\begin{bmatrix}\\beta_1 \\\\ \\beta_2  \\\\ \\vdots \\\\ \\beta_m\\end{bmatrix}  \\quad X = \\begin{bmatrix}X_{11} & X_{12} & \\cdots & X_{1m}\\\\ X_{21} & X_{22} & \\cdots & X_{2m}  \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ X_{n1} & X_{n2} & \\cdots & X_{nm}\\end{bmatrix}$$\n","\n","El encontrar la matriz de los parámetros estimados $\\hat{\\beta}$ para generar nuestra predicción $\\hat{Y}$ se lleva a cabo minimizando la suma de los errores para cada una de las variables presentes en nuestro set de datos. Esto nos genera una solución fácilmente programable en Numpy, de la forma:\n","\n","$$\\hat{\\beta} = (X^TX)^{-1}X^TY $$\n","\n","Para poder incorporar el intercepto ($\\beta_0$), tenemos que hacer una pequeña modificación a la matriz $X$, y esta corresponde a agregar una columna de largo $n$, con la constante $1$. Esto se hace para poder modelar el intercepto en la última ecuación presentada:\n","\n","$$X = \\begin{bmatrix}X_{11} & X_{12} & \\cdots & X_{1m}\\\\ X_{21} & X_{22} & \\cdots & X_{2m}  \\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ X_{n1} & X_{n2} & \\cdots & X_{nm}\\end{bmatrix} \\rightarrow X' = \\begin{bmatrix} 1& X_{11} & X_{12} & \\cdots & X_{1m}\\\\ 1 & X_{21} & X_{22} & \\cdots & X_{2m}  \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\ 1 &X_{n1} & X_{n2} & \\cdots & X_{nm}\\end{bmatrix}$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmD2ra0w6sYT"},"outputs":[],"source":["import pandas as pd\n","# Cargamos los datos desde Drive\n","cerveza_df = pd.read_csv(\"/content/drive/MyDrive/Becas_Capital_Humano_23/Material_clases_CD_AD_2023/M03-Analisis_exploratorio_estadistica/data/Consumo_cerveja.csv\")\n","cerveza_df.head(10).T # Mostramos los primeros 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwy9VEeM6sYT"},"outputs":[],"source":["#Extraemos nuestras primeras 5 variables independientes\n","X1 = cerveza_df['Temperatura Media (C)']\n","X2 = cerveza_df['Temperatura Minima (C)']\n","X3 = cerveza_df['Temperatura Maxima (C)']\n","X4 = cerveza_df['Precipitacao (mm)']\n","X5 = cerveza_df['Final de Semana']\n","\n","# Extraemos la variable dependiente\n","y = cerveza_df['Consumo de cerveja (litros)']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fw1iRd9d6sYT"},"outputs":[],"source":["# Vamos a graficar nuestra variable y a traves del tiempo:\n","import matplotlib.pyplot as plt\n","import numpy as np\n","plt.figure(figsize=(20,5))\n","plt.plot(np.arange(len(y)),y)\n","plt.title(\"Consumo de cerveza promedio diario\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"FtV_02sU6sYT"},"source":["Acá generaremos una grilla de matplotlib, la cual  nos permitirá generar subgráficos que ocupen más de una fila/columna.\n","\n","[Documentación Gridspec](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.gridspec.GridSpec.html)\n","\n","En este caso, vamos a hacer varios diagramas de dispersión, para ver la relación existente entre nuestras variables independientes, con la variable dependiente.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zTb8r3d26sYT"},"outputs":[],"source":["grid = plt.GridSpec(3, 2, wspace=0.2, hspace=0.3, height_ratios=[1, 1, 2])\n","\n","plt.figure(figsize=(12,10))\n","ax = plt.subplot(grid[0,0]) #Ocupar la primera fila, primera columna\n","ax.scatter(X1, y, alpha=0.5)\n","ax.set_title(\"$x_1$ Temperatura Media (C)\")\n","\n","ax = plt.subplot(grid[0,1])#Ocupar la primera fila, segunda columna\n","ax.scatter(X2, y, alpha=0.5)\n","ax.set_title(\"$x_2$ Temperatura Mínima (C)\")\n","\n","ax = plt.subplot(grid[1,0]) #Ocupar la segunda fila, primera columna\n","ax.scatter(X3, y, alpha=0.5)\n","ax.set_title(\"$x_3$ Temperatura Máxima (C)\")\n","\n","ax = plt.subplot(grid[1,1])#Ocupar la segunda fila, segunda columna\n","ax.scatter(X4, y, alpha=0.5)\n","ax.set_title(\"$x_4$ Precipitaciones (mm)\")\n","\n","ax = plt.subplot(grid[2,:])#Ocupar la tercera fila y ambas columnas\n","ax.scatter(X5, y, alpha=0.5)\n","ax.set_title(\"$x_5$ Fin de Semana\")\n","\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"XbKZ_lFg6sYT"},"source":["Podemos ver que el consumo de cerveza al parecer tiene una correlación positiva con las temperaturas. Generemos nuestra matriz X previo al proceso de obtención de los betas:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaWoQfix6sYT"},"outputs":[],"source":["X = np.zeros((len(X1),6))   # Generamos una matriz de Nx6, uno para cada una de\n","                            # nuestros variables independientes, e incluyendo la\n","                            # columna para el intercepto:\n","X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WYNjY7S36sYT"},"outputs":[],"source":["# Llenamos la matriz, con cada una de los valores correspondientes, en este caso,\n","# el intercepto ira en la primera columna y para el resto de las columnas las\n","# extraeremos utilizando los valores de una serie de pandas (.value)\n","X[:,0] = 1\n","X[:,1] = X1.values\n","X[:,2] = X2.values\n","X[:,3] = X3.values\n","X[:,4] = X4.values\n","X[:,5] = X5.values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQn6vEG16sYT"},"outputs":[],"source":["X[:5] # Ahora imprimimos las primeras 5 filas"]},{"cell_type":"markdown","metadata":{"id":"tjgZ33V56sYT"},"source":["Para transponer una matriz de Numpy, es tan sencillo como llamar a `matriz.T` para realizar esta operación. En el caso de la multiplicación matricial, necesitamos utilizar el método `np.dot` (Recordar que existen la restricción de que el número de columnas de la primera matriz, tiene que ser igual al número de filas de la segunda matriz). Finalmente, para invertir una matriz, tenemos que utilizar el método `np.lingal.inv` y aplicarlo en una matriz.\n","\n","$$ \\hat{\\beta} = (X^TX)^{-1}X^TY $$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5UgcJRQM6sYT"},"outputs":[],"source":["XTX = np.dot(X.T,X) # Calculamos X traspuesto multiplicado por X\n","XTY = np.dot(X.T,y) # Calculamos X traspuesto por Y\n","betas = np.dot(np.linalg.inv(XTX),XTY)  # Calculamos XTX^-1 por XTY, el resultado\n","                                        # va a ser nuestro vector de betas de largo m + 1\n","\n","# Extraemos los betas\n","beta_0 = betas[0]\n","beta_1 = betas[1]\n","beta_2 = betas[2]\n","beta_3 = betas[3]\n","beta_4 = betas[4]\n","beta_5 = betas[5]\n","\n","betas #Imprimimos los valores de nuestros parametros ajustados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qbpl3DaB6sYT"},"outputs":[],"source":["# Finalmente, definimos nuestra función  para la regresión lineal, que reciba\n","# por parámetros los arreglos para cada variable y que internamente, calcule la\n","# variable Y con los nuevos betas\n","\n","def lin_reg2(x1, x2, x3, x4, x5):\n","    y_hat = (beta_1 * x1) + (beta_2 * x2) + (beta_3 * x3) + (beta_4 * x4) + (beta_5 * x5) + beta_0\n","    return y_hat"]},{"cell_type":"markdown","metadata":{"id":"H4k37JIs6sYT"},"source":["Graficamos la señal original, en comparación con lo que obtenemos de nuestra regresión lineal múltiple"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nrxs_0ay6sYT"},"outputs":[],"source":["plt.figure(figsize=(20,5))\n","plt.plot(np.arange(len(y)), y, label=\"Original\")\n","plt.plot(np.arange(len(y)), lin_reg2(X1, X2, X3, X4, X5),'--', color=\"red\", label=\"Generados\")\n","\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0DPQiYey6sYT"},"source":["Y a su vez, podemos calcular nuestras métricas de bondad de ajuste, y errores asociados:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k53H-ur_6sYT"},"outputs":[],"source":["SST = np.sum((y - np.mean(y))**2)\n","SSE = np.sum((y - lin_reg2(X1,X2,X3,X4,X5))**2)\n","\n","R2 = 1 - (SSE/SST)\n","\n","print(f'\\nR2 Score = {R2:6.2f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKQ4n4YF6sYT"},"outputs":[],"source":["mse = np.sum((y - lin_reg2(X1,X2,X3,X4,X5))**2) / len(y)\n","mae = np.sum(np.abs(y - lin_reg2(X1,X2,X3,X4,X5))) / len(y)\n","mape = np.sum(np.abs((y - lin_reg2(X1,X2,X3,X4,X5))/y)) / len(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9NkPtsKq6sYT"},"outputs":[],"source":["print(f'\\nMSE = {mse:6.2f}\\nMAE = {mae:6.2f}\\nMAPE = {mape:5.2f}')"]},{"cell_type":"markdown","metadata":{"id":"FATtvcbA6sYT"},"source":["De lo obtenido, podemos ver que utilizando una regresión lineal múltiple, nuestro $R^2$ aumenta, y los errores disminuyen al compararse con el mejor de las regresiones simples realizadas en la dinámica 2."]},{"cell_type":"markdown","metadata":{"id":"l1Bu3Jad6sYT"},"source":["Para ir finalizando, existen múltiples librerías que ya tienen implementado las regresiones lineales, vamos a ver la implementación existente en `scikit-learn`\n","\n","[Documentación LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3WApiLq6sYT"},"outputs":[],"source":["# En este caso, de scikit-learn, de los modelos lineales, importamos la regresión lineal\n","from sklearn.linear_model import LinearRegression\n","lr = LinearRegression() # Y generamos un obtejo de LinearRegressión"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSP-lyZb6sYT"},"outputs":[],"source":["# Obtenemos nuestra matriz X\n","# Eliminamos las columnas 0 y 6 (fecha y consumo)\n","X = cerveza_df.iloc[:,1:6]\n","X.head()"]},{"cell_type":"markdown","metadata":{"id":"zHzT_b6j6sYT"},"source":["Una de las ventajas de scikit-learn, es que para la regresión lineal automáticamente añade el intercepto a calcular, sin que nosotros tengamos que modificar nuestra matriz $X$. Para poder ajustar nuestro modelo, necesitamos utilizar el método `lr.fit(X,y)`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pL-sdgTb6sYT"},"outputs":[],"source":["lr.fit(X, y.values)"]},{"cell_type":"markdown","metadata":{"id":"vB5Ih3NS6sYT"},"source":["Con esto, ya tenemos ajustado el modelo de regresión lineal, y solo queda extraer los betas. Para esto, tenemos que acceder a los parámetros `lr.intercept` (para $\\hat{\\beta_0}$) y `lr.coef_` para el resto de los betas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYodF_Gi6sYT"},"outputs":[],"source":["lr.intercept_, lr.coef_"]},{"cell_type":"markdown","metadata":{"id":"7bGCyCot6sYT"},"source":["Haciendo la comparación con nuestros betas, vemos que son iguales."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzUlPIXj6sYT"},"outputs":[],"source":["betas"]},{"cell_type":"markdown","metadata":{"id":"zANyavnX6sYT"},"source":["### <font color='green'>**Actividad 3 (resuelta)**</font>\n","### Regresión lineal múltiple, para predecir el precio de una casa.\n","\n","En este caso, trabajaremos con un set de datos de precios de casas del condado de King, Georgia, USA. El set de datos contiene 21 variables, pero no todas van a servir para el proceso de la regresión. Su tarea va a ser seleccionar las variables que va a descartar, y trabajar con el resto de las variables restantes para generar una regresión lineal múltiple utilizando Numpy para obtener los $\\hat{\\beta}$. Además de esto, calcule el $R^2$ y obtenga las métricas de error $MSE$, $MAE$ y $MAPE$.\n","\n","**Desafío:** Puede que el uso de todas las variables no genere el mejor modelo. Por lo mismo, el **desafío** consiste en implementar el algoritmo de __Step Forward Feature Selection__. La idea de este algoritmo es agregar de a 1 en 1 las variables existentes en nuestro set de datos y ver como mejora el proceso de ajuste de la regresión lineal.\n","\n","1. De las posibles variables consideradas en un principio (la cual llamaremos $m$), entrene $m$ modelos de regresión lineal (uno por cada variable), y seleccione la variable que obtenga un mejor $MAE$.\n","\n","2. Con la variable seleccionada, remuévala del conjunto de posibles variables y guárdela en una lista de variables seleccionadas (`selected_vars`)\n","\n","3. Del conjunto de posibles variables restantes, entrene $m-len(selected\\_vars)$ modelos de regresión lineal utilizando las variables restantes en conjunto con `selected_vars`. Seleccione la nueva variable que obtenga un mejor $MAE$. Repita el paso 2. hasta que no quede ninguna variable.\n","\n","4. Finalmente, con el procedimiento realizado, y ninguna variable restante en el conjunto de posible variable, imprima por pantalla el mejor conjunto de variables junto con su $MAE$. Adicionalmente, puede graficar la curva de cómo va evolucionando el $MAE$ a medida que va agregando variables.\n","\n","**Nota:** Para el desafío, es recomendable utilizar `LinearRegression` de `scikit-learn` por temas de facilidad de implementación del algoritmo de __Step Forward Feature Selection__. Para poder obtener el $\\hat{Y}$ utilizando `LinearRegression`, se debe utilizar el método `.predict(X)` posteriormente a haber ajustado el modelo (`.fit(X,y)`), y este computara:\n","\n","$$\\hat{Y}=\\hat{\\beta_0}+X_1\\hat{\\beta_1}+X_2\\hat{\\beta_2}+\\dots+X_m\\hat{\\beta_m}$$\n","\n","Ejemplo de uso:\n","\n","```python\n","lr = LinearRegression()\n","lr.fit(X,y)\n","y_hat = lr.predict(X)\n","```\n","\n","\n","Archivo: `kc_house_data.csv`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1L_3T5__-iQk"},"outputs":[],"source":["# Su código aca ...\n","\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KUqh0j996sYT"},"outputs":[],"source":["# Drive\n","housing_df = pd.read_csv(\"/content/drive/MyDrive/Becas_Capital_Humano_23/Material_clases_CD_AD_2023/M03-Analisis_exploratorio_estadistica/data/kc_house_data.csv\")\n","housing_df.head().T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JLCul1S6sYT"},"outputs":[],"source":["housing_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xQPO80m-xyh"},"outputs":[],"source":["y = housing_df['price'].values\n","X = housing_df.iloc[:,3:].values\n","X"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_pyfScQWTqc"},"outputs":[],"source":["housing_col_names = housing_df.columns[3:].values\n","housing_col_names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nIg68kT2WTqd"},"outputs":[],"source":["# Veamos el shape de ambas\n","X.shape, y.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tb8RhJf2-zhi"},"outputs":[],"source":["lr = LinearRegression()\n","\n","lr.fit(X, y)\n","y_hat = lr.predict(X)\n","print('Scores', r2_score(y,y_hat), mean_squared_error(y,y_hat), mean_absolute_error(y,y_hat))\n","\n","#print(lr.intercept_, lr.coef_)"]},{"cell_type":"code","source":["X"],"metadata":{"id":"PDJFMZQMfiKh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### __Versión iterativa__"],"metadata":{"id":"ub7ooPR_Zje2"}},{"cell_type":"code","source":["from sklearn import metrics\n","# Inicializamos una semilla\n","np.random.seed(42)\n","\n","# Generamos una lista con los índices de todas las columnas de X\n","restantes = list(range(X.shape[1]))\n","\n","# Generamos un entero entre 0 y el número de columnas de X\n","primero = np.random.randint(0,X.shape[1])\n","actuales = []\n","# Seleccionamos dicha variable\n","actuales.append(restantes.pop(primero))\n","\n","# Generamos la regresión y evaluamos el error\n","lr = LinearRegression()\n","\n","lr.fit(X[:,actuales], y)\n","y_hat = lr.predict(X[:,actuales])\n","error_actual = mean_squared_error(y,y_hat)\n","err = [error_actual]\n","\n","# ientras el arreglo tenga alguna variable\n","while restantes:\n","    error_att = []\n","    # Evaluamos cada atributo\n","    for atributo_candidato in restantes:\n","        # generamos un arreglo candidato\n","        nuevas_variables = actuales +[atributo_candidato]\n","        # Ajustamos la regresión y hacemos inferencia\n","        model = LinearRegression()\n","        model.fit(X[:,nuevas_variables], y)\n","        pred = model.predict(X[:,nuevas_variables])\n","        # Guardamos el MSE\n","        error_att.append(metrics.mean_squared_error(y,pred))\n","    # Obtenemos el menor error\n","    min_error = np.min(error_att)\n","    #s eleccionamos dicho índice el cual corresponda al menor error\n","    print(f\"Seleccionado:{restantes[error_att.index(min_error)]} MSE:{min_error}\")\n","    print(\"=====\")\n","    # Seleccionamos la variable\n","    actuales.append(restantes[error_att.index(min_error)])\n","    restantes.pop(error_att.index(min_error))\n","    # Guardamos el error acumulado por iteración de selección\n","    err.append(min_error)\n","\n"],"metadata":{"id":"GvBIKu-4YiUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X[:,[3]]"],"metadata":{"id":"oFjeFJJVisTx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VyUD6qKDWTqd"},"source":["### __Versión con `sklearn`__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8tMRi1ZUWTqd"},"outputs":[],"source":["lreg = LinearRegression()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lEfOupjrWTqe"},"outputs":[],"source":["from sklearn.feature_selection import SequentialFeatureSelector\n","sfs1 = SequentialFeatureSelector(lreg,\n","                                 n_features_to_select=4,\n","                                 cv=None,\n","                                 direction=\"forward\").fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMwXLOsTWTqe"},"outputs":[],"source":["print(housing_col_names[sfs1.get_support()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-Ze133pWTqe"},"outputs":[],"source":["X_sfs = sfs1.transform(X)\n","X_sfs.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zjz_9L5sWTqe"},"outputs":[],"source":["lr2 = LinearRegression()\n","\n","lr2.fit(X_sfs, y)\n","y_hat2 = lr2.predict(X_sfs)\n","print('Scores', r2_score(y,y_hat2), mean_squared_error(y,y_hat2), mean_absolute_error(y,y_hat2))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UI_cesQeWTqe"},"outputs":[],"source":["sfs1.get_params(deep=True)"]},{"cell_type":"markdown","metadata":{"id":"dfp6vtJbWTqf"},"source":["### __Versión con `mlxtend`__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0XMGklfWTqf"},"outputs":[],"source":["from mlxtend.feature_selection import SequentialFeatureSelector as sfs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64gCKGREWTqf"},"outputs":[],"source":["lr3 = LinearRegression()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SiLFRhT_WTqg"},"outputs":[],"source":["# Configuración de sfs\n","sfs1 = sfs(lr3,\n","           k_features=7,\n","           forward=True,\n","           cv=5, # Número de divisipones para el Cross Validation\n","           verbose=2,\n","           scoring=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nCd41wO7WTqh"},"outputs":[],"source":["X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1TiAYPZqWTqh"},"outputs":[],"source":["# ajustar sfs\n","\n","sfs1 = sfs1.fit(X, y) #, custom_feature_names=list(housing_col_names))"]},{"cell_type":"code","source":["# Obtener los índices de las características seleccionadas\n","selected_indices = list(sfs1.k_feature_idx_)\n","\n","# Mapear los índices a los nombres de las características\n","selected_feature_names = [housing_col_names[i] for i in selected_indices]\n","\n","# Mostrar resultados\n","print('Índices de características seleccionadas:', selected_indices)\n","print('Nombres de características seleccionadas:\\n', *selected_feature_names, sep='\\n')"],"metadata":{"id":"bEoGyFABO0ZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BR6zdzrxWTqh"},"outputs":[],"source":["# probemos con k_features = 18\n","sfs2 = sfs(lr3,\n","           k_features=18,\n","           forward=True,\n","           cv=5,\n","           verbose=2,\n","           scoring=None)\n","sfs2 = sfs2.fit(X, y)"]},{"cell_type":"code","source":["# Obtener los índices de las características seleccionadas\n","selected_indices = list(sfs2.k_feature_idx_)\n","\n","# Mapear los índices a los nombres de las características\n","selected_feature_names = [housing_col_names[i] for i in selected_indices]\n","\n","# Mostrar resultados\n","print('Índices de características seleccionadas:', selected_indices)\n","print('Nombres de características seleccionadas:\\n', *selected_feature_names, sep='\\n')"],"metadata":{"id":"b0hoPWDcPfBk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5INxLlBMWTqh"},"outputs":[],"source":["pd.set_option('display.float_format', '{:.5f}'.format)\n","\n","dict_data = sfs1.get_metric_dict()\n","df = pd.DataFrame.from_dict(dict_data).T\n","df.loc[:, ['feature_idx', 'avg_score', 'ci_bound', 'std_dev', 'std_err']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7RYBDeJWTqh"},"outputs":[],"source":["pd.set_option('display.float_format', '{:.5f}'.format)\n","dict_data = sfs2.get_metric_dict()\n","df = pd.DataFrame.from_dict(dict_data).T\n","df.loc[:, ['feature_idx', 'avg_score', 'ci_bound', 'std_dev', 'std_err']]"]},{"cell_type":"code","source":["pd.reset_option('display.float_format')"],"metadata":{"id":"3QGl2mZhT2EG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uwzaN53c6sYT"},"source":["<font color='green'>Fn actividad 3</font>"]},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?export=view&id=1Igtn9UXg6NGeRWsqh4hefQUjV0hmzlBv\" width=\"50\" align=\"left\" title=\"Runa-perth\">\n","<br clear=\"left\">\n","Fin contenido opcional"],"metadata":{"id":"qOlasOrlUHbQ"}},{"cell_type":"code","source":[],"metadata":{"id":"-v14eglHUILg"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}