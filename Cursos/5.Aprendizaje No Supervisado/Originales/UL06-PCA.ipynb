{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vChJKs1NQpf9"},"source":["# **Aprendizaje no supervisado**\n","# UL06. Principal component analysis (PCA) - SOLUCIÓN"]},{"cell_type":"markdown","metadata":{"id":"4otZbqVmnOEW"},"source":["## <font color='blue'>**Métodos lineales de reducción**</font>\n","\n","En este notebook, se presentará el  Análisis de componentes principales (PCA) para reducción de dimensionalidad y extracción de características.\n","\n","El método tiene implementaciones listas para usar en scikit-learn que son útiles para sus proyectos, pero para el propósito de este notebook  es implementar el métodos desde cero, usando solo OpenCV para abrir y guardar imágenes, y NumPy para manejar matrices.\n","\n","El objetivo de PCA es obtener un subespacio (es decir, unas dimensiones más bajas) y proyectar la nube de puntos de datos al subespacio sin perder información incrustada en los datos originales con dimensiones más altas, por lo que busca un vector que maximice la variación de los datos."]},{"cell_type":"markdown","metadata":{"id":"vHl-OWw5nvJA"},"source":["## <font color='blue'>**Principal component analysis (PCA)**</font>\n","\n","Si se pregunta por qué PCA es útil, estos son  los 3 principales beneficios:\n","\n","1. Reduce el tiempo de entrenamiento, debido a un conjunto de datos más pequeño\n","2. Elimina el ruido al mantener solo lo relevante\n","3. Hace posible la visualización, en los casos en que tiene un máximo de 3 componentes principales\n","\n","Principal Component Analysis (PCA) es un método estadístico que permite simplificar la complejidad de espacios muestrales con muchas dimensiones a la vez que conserva su información. Supóngase que existe una muestra con $n$ individuos cada uno con $p$ variables aleatorias $(X_1, X_2, …, X_p)$, es decir, el espacio muestral tiene $p$ dimensiones. PCA permite encontrar un número de factores subyacentes ($z<p$) que explican aproximadamente lo mismo que las p variables originales. Donde antes se necesitaban $p$ valores para caracterizar a cada individuo, ahora bastan $z$ valores. Cada una de estas $z$ nuevas variables recibe el nombre de componente principal. Esta tecnica pertenece a la familia de aprendizaje no supervisado.\n","\n","PCA aborda esta tarea mediante la identificación de componentes principales, que son combinaciones lineales de las características originales. Estos componentes se extraen de modo que el primer componente principal encapsula la varianza máxima en el conjunto de datos, el segundo encapsula la varianza restante mientras no está correlacionado con el primero, y así sucesivamente.\n"]},{"cell_type":"markdown","metadata":{"id":"FlXYqHHzpXKM"},"source":["## La intuición\n","\n","¿Como proyectamos a una dimensión maximizando la varianza?\n","\n","![Intuición1](https://drive.google.com/uc?export=view&id=1RgWr0oVb1ctZ_PlNBI-YbvpWQbxkuetY)\n","\n","¿Y en este caso?\n","\n","![Intuición1](https://drive.google.com/uc?export=view&id=1IZcDhrZ2S4YEnxIvvxUp4iUmlkTzNYEK)"]},{"cell_type":"markdown","metadata":{"id":"ilzzf3pkwaTi"},"source":["Esto lo podemos modelar como un problema de maximización:\n","\n","$$ \\max_{v_1} (\\frac{1}{n} \\sum_{i=1}^n (U_1^Tx_i - U_1^T\\bar{x})^2)$$ asumiendo $\\bar{x}=0$, se tiene\n","$$ \\max_{v_1} (\\frac{1}{n} \\sum_{i=1}^n (U_1^Tx_i)^2)$$, sujeto a $$ ||U_1||=1$$\n","\n","Esta aproximación se llama **variance maximization approach**.\n","\n","Al desarrollar el cuadrado obtenemos.\n","\n","$$ \\frac{1}{n}\\sum_{i=1}^n(U_1^Tx_i)^2= U_1^T\\frac{(X^TX)}{n}U_1$$,\n","Donde $\\frac{(X^TX)}{n}$ es la matriz de covarianza.\n","\n","\n","Si quiere seguir profundizando puede ver:\n","https://medium.com/analytics-vidhya/mathematics-behind-principal-component-analysis-pca-1cdff0a808a9\n","\n","\n","Otra forma de ver el problema es establecerlo como un problema de minimización.\n","\n","Otra forma de pensar en PCA es que se ajusta a la mejor línea que atraviesa nuestros datos con el objetivo de minimizar el error de proyección \"d\" para cada punto. Este enfoque se denomina enfoque de minimización de la distancia.\n","\n","$$d^2 = ||x_i||^2 - (U_1^Tx_i)^2$$\n","$$=(x_i^Tx_i)-(U_1^Tx_i)^2$$\n","\n","Entonces se quiere minimizar:\n","\n","$$ \\min_{U_1}\\sum_{i=1}^n (x_i^Tx_i)-(U_1^Tx_i)^2$$ Sujeto a\n","$$ ||U_1||=1$$\n","\n","La solución de estos problemas de optimización nos entregan las componentes principales.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Pcace03o3kFV"},"source":["## <font color='green'>**Actividad 1**</font>\n","\n","\n","El objetivo de esta actividad es aplicar el siguiente procedimiento para calcular las componentes principales y posteriormente visualizar las dos primeras componentes..\n","\n","\n","![Visualizando](https://drive.google.com/uc?export=view&id=1h7ea0kZsG4WIekE_CYAaFoeoF4EByTYz)\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uunDAMV7Za6G"},"source":["Vamos a analizar, un data set súper simple para aprender., Asi que vamos por con el conocido conjunto de datos Iris. Inicialmente tiene solo 4 características, aún imposible de visualizar y utilizaremos PCA para visualizar caracteristicas.\n","\n","![Iris](https://drive.google.com/uc?export=view&id=1wB6sEo2oIupsW3ZBEgj9CYotC-07m-Tv)\n","\n","Procedimiento:\n","\n","1. Cargue la la siguiente data y transformelo a numpy.\n","\n","```\n","df = pd.read_csv('https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv')\n","```\n","\n","2. Visualice la data y observe que tienen distintos valores. Por lo tanto es necesario normalizar la data. Utilice\n","\n","```\n","from sklearn.preprocessing import StandardScaler\n","```\n","3. Calcule la matriz de covariaza sobre las features.\n","\n","```\n","features = X_scaled.T\n","cov_matrix = np.cov(features)\n","```\n","\n","4. Calcule los autovalores, autovectores y el contenido de información de cada componente.\n","\n","\n","\n","```\n","print(100*eig_val_cov/eig_val_cov.sum())\n","```\n","\n","5. Proyecte las dos primeras componentes.\n","\n","```\n","projected_1 = X_scaled.dot(eig_vec_cov.T[0])\n","```\n","\n","6. Visualice las dos primeras componentes.\n","\n","7. Compare el resultado con pca de sklearn.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"jcbGpXRurHTa"},"source":["import numpy as np\n","import pandas as pd\n","df = pd.read_csv('https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv')\n","\n","X = df.to_numpy()\n","print(X[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pb8_hHvva-Pc"},"source":["<font color='green'>Fin Actividad 1</font>"]},{"cell_type":"markdown","metadata":{"id":"bDnbJAGqdyxE"},"source":["## <font color='green'>Actividad 2</font>"]},{"cell_type":"markdown","metadata":{"id":"azY_dHzN0NKy"},"source":["## Compresión de una imagen a través del uso de PCA.\n","\n","Determine la cantidad de componentes necesarias para obtener una calidad adecuada. Primero realice un PCA y posteriormente invierta la transformación con las componentes seleccionadas.\n","\n","\n","```\n","import numpy as np\n","import glob\n","import cv2\n","path1 = 'ruta imagenes'\n","images = []\n","for path in glob.glob(path1+\"test/*.jpg\"):\n","    images.append(cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2GRAY))\n","```\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_25nzNKWgPZ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FCOshl_npBhE"},"source":["# Leemos nuestras imagenes\n","import numpy as np\n","import glob\n","import cv2\n","path1 = '/content/drive/MyDrive/Becas_Capital_Humano/Material/M05-Apredizaje_No_supervisado/data/'\n","images = []\n","for path in glob.glob(path1+\"/*.jpg\"): #Leemos todas las imagenes\n","  images.append(cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2GRAY))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JFWZypMb1vyA"},"source":["#Trabajaremos solo con la primera\n","print(images[0].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1d5MXZhrbeu2"},"source":["#TIP\n","#import matplotlib.pyplot as plt\n","#plt.figure(figsize=[12,8])\n","#lt.imshow(image_recon,cmap = plt.cm.gray)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b_tAkyzBeSWe"},"source":["<font color='green'>Fin Actividad 2</font>"]},{"cell_type":"markdown","metadata":{"id":"6vbOzHmgeAyd"},"source":["## <font color='green'>Actividad 3</font>"]},{"cell_type":"markdown","metadata":{"id":"FRMVlbx14fX8"},"source":["### Desafío 2: Realice esto mismo pero para el caso de una imagen color\n"]},{"cell_type":"markdown","metadata":{"id":"R73ojyPieD3T"},"source":["## <font color='green'>Actividad 4</font>\n","\n","Esta actividad tiene por objetivo extender los conceptos de PCA para el caso que existan comportamientos no lineales. Para esto se utilizará el concepto de Kernel utilizado en SVM.\n","\n","**Kernel PCA:** PCA es un método lineal. Es decir, solo se puede aplicar a conjuntos de datos que son linealmente separables. Pero, si lo usamos para conjuntos de datos no lineales, podríamos obtener un resultado que puede no ser la reducción de dimensionalidad óptima. Kernel PCA utiliza una función de kernel para proyectar el conjunto de datos en un espacio de características de mayor dimensión, donde es linealmente separable. Es similar a la idea de Support Vector Machines.\n","\n","Hay varios métodos de kernel como lineal, polinomial y gaussiano.\n","\n","![Visualizando](https://drive.google.com/uc?export=view&id=1IjMqDaBhF60Y4px5dKzuuGeE7eI4Ey8V)\n","\n","En la imagen anterior, los datos estan en dos dimensiones. Los denotaremos por $x=(x_1,x_2)$. EN su forma original, los datos son inseparables en su espacio. Sin emabrgo a traves de la aplicación de una funcion kernel lo podemos transformar para ser separable linealmente.\n","\n","$$ \\phi \\rightarrow x_1^2,x_2^2,\\sqrt{2}x_1x_2$$"]},{"cell_type":"markdown","metadata":{"id":"1SV_S3W3frVM"},"source":["1. Utilice el conjunto de datos make_moons, genere un dataset y visualice su forma.\n","\n","2. Aplique el método de PCA y visualice sus componentes.\n","\n","3. Aplique el método de kernel PCA y visualice sus componentes.\n","\n","4. Genere alguna conclusión."]},{"cell_type":"code","metadata":{"id":"0d_G8LQXfrpy"},"source":["import matplotlib.pyplot as plt\n","from sklearn.datasets import make_moons\n","\n","X, y = make_moons(n_samples = 500, noise = 0.02, random_state = 417)\n","\n","plt.scatter(X[:, 0], X[:, 1], c = y)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dvABqZliecNR"},"source":["<font color='green'>Fin Actividad 4</font>"]},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?export=view&id=1bqkJJ7QiIOTsp-7jm5eUtK-XYWMEq2_K\" width=\"100\" align=\"left\" />\n","<br clear=\"left\">\n","\n","## <font color='blue'>**Resumen**</font>\n","\n","PCA es una técnica de reducción de dimensionalidad utilizada en el análisis de datos y el aprendizaje automático. Su objetivo principal es transformar un conjunto de datos con múltiples características en un conjunto de dimensiones más bajas mientras se conserva la mayor cantidad posible de variabilidad.\n","\n","Funcionamiento:\n","\n","1. Identificación de componentes: PCA busca las direcciones (componentes principales) a lo largo de las cuales los datos tienen la mayor variabilidad.\n","\n","2. Proyección: Los datos originales se proyectan en el espacio definido por los componentes principales.\n","\n","3. Selección de componentes: Se pueden seleccionar los componentes principales más significativos o reducir la dimensionalidad manteniendo solo los primeros componentes.\n","\n","Ventajas de PCA:\n","\n","1. Reducción de dimensionalidad: Permite reducir la cantidad de características sin perder mucha información, lo que simplifica el análisis y acelera los algoritmos de aprendizaje automático.\n","\n","2. Eliminación de correlación: PCA transforma las características correlacionadas en componentes no correlacionados, lo que puede mejorar la interpretación de los datos.\n","\n","3. Visualización: PCA permite visualizar datos en dimensiones reducidas para entender mejor la estructura de los datos.\n","\n","Desventajas de PCA:\n","\n","1. Pérdida de interpretabilidad: Los componentes principales son combinaciones lineales de características originales, lo que puede dificultar la interpretación de los nuevos ejes.\n","\n","2. Sensibilidad a escalas: La escala de las características puede afectar el resultado de PCA, por lo que es necesario escalar los datos antes de aplicarlo.\n","\n","3. No linealidad: PCA es adecuado para identificar patrones lineales en los datos, pero puede no ser efectivo para relaciones no lineales.\n","\n","Ejemplos de aplicaciones de PCA:\n","\n","1. Análisis de imágenes: En el procesamiento de imágenes, PCA se usa para reducir la cantidad de píxeles y extraer características principales para reconocimiento de objetos.\n","\n","2. Reconocimiento de patrones: En aprendizaje automático, PCA puede utilizarse como preprocesamiento para reducir la dimensionalidad de los datos antes de entrenar modelos de clasificación.\n","\n","3. Biometría: En aplicaciones de reconocimiento facial, PCA puede ayudar a reducir la cantidad de características faciales y mejorar la precisión del reconocimiento.\n","\n","4. Análisis de datos financieros: En finanzas, PCA se utiliza para identificar las principales tendencias y factores que afectan los rendimientos de activos financieros.\n","\n","En resumen, PCA es una técnica poderosa para reducir la dimensionalidad de los datos, lo que puede mejorar la eficiencia y rendimiento de los modelos de aprendizaje automático. Sin embargo, es importante considerar sus limitaciones y seleccionar adecuadamente el número de componentes principales para cada aplicación."],"metadata":{"id":"4wvO9tSujwUM"}}]}