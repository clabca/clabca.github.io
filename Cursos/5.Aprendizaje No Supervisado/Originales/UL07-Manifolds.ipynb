{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RqhsjpC-T_Ff"},"source":["# **Aprendizaje no supervisado**\n","# UL07. Manifolds - SOLUCIÓN"]},{"cell_type":"markdown","metadata":{"id":"EAnLk1hzlbli"},"source":["## <font color='blue'>**Manifolds Learning**</font>\n","\n","Las distancias euclidianas pueden no funcionar para ciertos problemas debido a la geometría de los datos. Sin embargo, existe una serie de otras métricas que nos permite  superar esto, como por ejemplo  la distancia geodésica. ISOMAP es una de las variedades de aprendizaje más famosas, utiliza la distancia geodésica para comprender los datos.\n","\n","## El concepto de manifolds\n","\n","![Visualizando](https://drive.google.com/uc?export=view&id=1epAR6zE8u5vX_ETE_rEZphdwQVKbjqGd)\n","\n","Localmente, la circunferencia parece una línea, que tiene una sola dimensión. En otros términos, una sola coordenada es suficiente para describir un pequeño arco de circunferencia. Consideremos, por ejemplo, la parte superior de la circunferencia, para la que la coordenada y es positiva (la parte amarilla en la figura 1). Cualquier punto de esta parte puede ser descrito por la coordenada x. Existe, por lo tanto, un homeomorfismo entre la parte amarilla de la circunferencia y el intervalo abierto [−1, 1] que representa cada punto de la circunferencia por su primera coordenada.\n","\n","**Una variedad (Manifolds) n** es un espacio topológico que, en cada punto, tiene una vecindad que es homeomorfa al espacio euclidiano n-dimensional.\n","\n","En topología, un **homeomorfismo**  es una función de un espacio topológico a otro, que cumple con ser una función biyectiva continua y cuya inversa es continua. En este caso, los dos espacios topológicos se dicen homeomorfos. Las propiedades de estos espacios que se conservan bajo homeomorfismos se denominan propiedades topológicas.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"faKReqH5ocbk"},"source":["### Isomap/Isometric mapping\n","\n","Es un algoritmo de aprendizaje de variedades que intenta preservar la distancia geodésica entre muestras mientras reduce la dimensión. Miremos un ejemplo concreto.\n","\n","\n","<img src='https://drive.google.com/uc?export=view&id=1hPMnNTJhnzlxLxEkfEhZ6K3PI40qq2Oo' width=\"300\" align=\"left\" style=\"margin-right: 20px\">\n","\n","<br clear=\"left\">\n","\n","En la figura anterior, los datos forman una forma de espiral (no lineal). La línea roja determina la distancia geodésica entre x1 y x2, mientras que la línea azul representa la distancia euclidiana.\n","\n","Como calculamos esta distancia.\n","\n","\n","<img src='https://drive.google.com/uc?export=view&id=1YlmbakUQPzzkvol-qMFpn0gmEKLkmSyE' width=\"300\" align=\"left\" style=\"margin-right: 20px\">\n","<br clear=\"left\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"I-w5cbJVyG74"},"source":["### t-SNE (T-distributed Stochastic Neighbor Embedding)\n","\n","Adaptado de  (https://www.youtube.com/watch?v=NEaUSP4YerM)\n","\n","¿Como podemos proyectar para generar una buena separación?\n","\n","<img src='https://drive.google.com/uc?export=view&id=1PTJq3rG7_pn6jkYoqopeqf2yc270MiNr' width=\"300\" align=\"left\" >\n","<br clear=\"left\">\n","\n","Supongamos que el grupo de puntos los pondemos en forma aleatoria sobre la recta.\n","\n","<img src='https://drive.google.com/uc?export=view&id=1_5FGWPnrJENgy7LzvmWkNkU3aibquwSx' width=\"300\" align=\"left\" style=\"margin-right: 20px\">\n","<br clear=\"left\">\n","\n","Posteriormente vamos moviendo los puntos en terminos de la similitud entre los datos.\n","\n","\n","<img src='https://drive.google.com/uc?export=view&id=142OstQ7Sixs_guSEHksHAILFBs7-2FrB' width=\"300\" align=\"left\" style=\"margin-right: 20px\">\n","<br clear=\"left\">\n","\n","¿Como medimos la similitud?, a traves de una matriz de similaridad.\n","\n","<img src='https://drive.google.com/uc?export=view&id=1niItSDu-U8ztya9VjkQ4Bjg2_W-o16lV' width=\"300\" align=\"left\" style=\"margin-right: 20px\">\n","<br clear=\"left\">\n","\n","\n","¿Como se construye la matriz?\n","\n","Mida las similitudes entre puntos en el espacio de alta dimensión. Piense en un montón de puntos de datos dispersos en un espacio 2D. Para cada punto de datos ($x_i$), centraremos una distribución gaussiana sobre ese punto. Luego medimos la densidad de todos los puntos ($x_j$) bajo esa distribución gaussiana. Luego renormalice para todos los puntos. Esto nos da un conjunto de probabilidades ($P_{ij}$) para todos los puntos. Esas probabilidades son proporcionales a las similitudes. Todo lo que eso significa es que, si los puntos de datos $x_1$ y $x_2$ tienen valores iguales bajo este círculo gaussiano, entonces sus proporciones y similitudes son iguales y, por lo tanto, tiene similitudes locales en la estructura de este espacio de alta dimensión.\n","\n","<br clear=\"left\">\n","<img src='https://drive.google.com/uc?export=view&id=1L_oOGBDRIUuNIaGFoZwz9Rv1rPrZUQXf' width=\"300\" align=\"left\" style=\"margin-right: 20px\">\n","\n","<br clear=\"left\">\n","<br clear=\"left\">\n","\n","<img src='https://drive.google.com/uc?export=view&id=16zTYNlFaCc0fcJRdKS2T4gE2Csm690lX' width=\"300\" align=\"left\" style=\"margin-right: 20px\">\n","<br clear=\"left\">\n","\n","\n","En lugar de usar una distribución gaussiana, usa una distribución t de Student con un grado de libertad, que también se conoce como distribución de Cauchy . Esto nos da un segundo conjunto de probabilidades ($Q_{ij}$) en el espacio de baja dimensión. Como puede ver, la distribución t de Student tiene colas más pesadas que la distribución normal. Las colas pesadas permiten un mejor modelado de distancias alejadas. Lo cual permite una mejor separación de los puntos.\n","\n","El último paso es que queremos que este conjunto de probabilidades del espacio de baja dimensión ($Q_{ij}$) refleje las del espacio de alta dimensión ($P_{ij}$) de la mejor manera posible. Queremos que las dos estructuras del mapa sean similares. Medimos la diferencia entre las distribuciones de probabilidad de los espacios utilizando la divergencia de Kullback-Liebler (KL). Finalmente, usamos el descenso de gradiente para minimizar nuestra función de costo KL.\n","\n","<img src='https://drive.google.com/uc?export=view&id=1NIcKCynfvYBlXriaSvHuvzCHqXIa5Q7Y' width=\"600\" align=\"Centered\" style=\"margin-right: 20px\">\n","<br clear=\"left\">\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mmXsPeitrMMx"},"source":["## <font color='green'>Actividad 1</font>\n","\n","Vamos a trabajar con el dataset de digitos.\n","\n","1. Carge la libreria de digitos y explorela.\n","2. Aplique los métodos de reducción de dimensionalidad t-SNE, PCA, Kernel-PCA y ISOMAP.\n","3. Visualice las primeras componentes y selecciones el metodo que genera la mejor separación. Comprenda y utilice la funcion plot_embedding la cual es proporcionada en el código.\n","4. Construya un modelo que prediga el valor de un digito.\n"]},{"cell_type":"code","metadata":{"id":"H-UXmbonleit"},"source":["from time import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import offsetbox\n","from sklearn import (manifold, datasets, decomposition, ensemble,\n","                     discriminant_analysis, random_projection, neighbors)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eN0p-zf3rUkE"},"source":["digits = datasets.load_digits(n_class=6)\n","X = digits.data\n","y = digits.target\n","n_samples, n_features = X.shape\n","n_neighbors = 30"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6rmc-raBrXMt"},"source":["# Escalar y visualizar los vectores de embeddings\n","def plot_embedding(X, title=None):\n","    x_min, x_max = np.min(X, 0), np.max(X, 0)\n","    # Estamos normalizando los datos.\n","    X = (X - x_min) / (x_max - x_min)\n","    plt.figure()\n","    ax = plt.subplot(111)\n","    for i in range(X.shape[0]):\n","      # Posición y valor del texto.\n","        plt.text(X[i, 0], X[i, 1], str(y[i]),\n","                 color=plt.cm.Set1(y[i] / 10.),\n","                 fontdict={'weight': 'bold', 'size': 9})\n","\n","    if hasattr(offsetbox, 'AnnotationBbox'):\n","\n","        shown_images = np.array([[1., 1.]])\n","        for i in range(X.shape[0]):\n","            dist = np.sum((X[i] - shown_images) ** 2, 1)\n","            if np.min(dist) < 4e-3:\n","                # No mostrar valores muy cercanos\n","                continue\n","            shown_images = np.r_[shown_images, [X[i]]]\n","            imagebox = offsetbox.AnnotationBbox(\n","                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n","                X[i])\n","            ax.add_artist(imagebox)\n","    plt.xticks([]), plt.yticks([])\n","    if title is not None:\n","        plt.title(title)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PP8Yy9S7rfp6"},"source":["n_img_per_row = 20\n","img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\n","for i in range(n_img_per_row):\n","    ix = 10 * i + 1\n","    for j in range(n_img_per_row):\n","        iy = 10 * j + 1\n","        img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))\n","\n","plt.imshow(img, cmap=plt.cm.binary)\n","plt.xticks([])\n","plt.yticks([])\n","plt.title('Una seleccion del dataset de digitos con 64 dimensiones (cada digito)')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SqH8qJVmrSvA"},"source":["<font color='green'>Fin Actividad 1</font>"]},{"cell_type":"markdown","metadata":{"id":"5wYnt2rTQT10"},"source":["## <font color='green'>Actividad 2</font>\n","Una aplicación de las diferentes técnicas de aprendizaje en variedades a un conjunto de datos esféricos. Aquí se puede ver el uso de la reducción de dimensionalidad para ganar algo de intuición sobre los algunos métodos de aprendizaje en variedades."]},{"cell_type":"code","metadata":{"id":"bxttNUYxQvl0"},"source":["from time import time\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from matplotlib.ticker import NullFormatter\n","from sklearn import manifold\n","from sklearn.utils import check_random_state\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AYk5ZUyYQzNK"},"source":["# Create our sphere.\n","Axes3D\n","\n","# Variables for manifold learning.\n","n_neighbors = 10\n","n_samples = 1000\n","random_state = check_random_state(0)\n","p = random_state.rand(n_samples) * (2 * np.pi - 0.55)\n","t = random_state.rand(n_samples) * np.pi\n","\n","# Sever the poles from the sphere.\n","indices = ((t < (np.pi - (np.pi / 8))) & (t > ((np.pi / 8))))\n","colors = p[indices]\n","x, y, z = np.sin(t[indices]) * np.cos(p[indices]), \\\n","    np.sin(t[indices]) * np.sin(p[indices]), \\\n","    np.cos(t[indices])\n","\n","# Plot our dataset.\n","fig = plt.figure(figsize=(15, 8))\n","plt.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n","             % (1000, n_neighbors), fontsize=14)\n","\n","ax = fig.add_subplot(251, projection='3d')\n","ax.scatter(x, y, z, c=p[indices], cmap=plt.cm.rainbow)\n","ax.view_init(40, -10)\n","\n","sphere_data = np.array([x, y, z]).T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gLVCRwJkH82-"},"source":["### Aplique los metodos t-SNE, ISOMAP y PCA a un dataset esférico\n","\n","1. Transforme el dataset\n","2. Visualice las primeras dos componentes en un grafico tipo scatter.\n","3. Discuta los resultados. Evalue visualmente el resultado y comente los tiempos de ejecución de cada tecnica."]},{"cell_type":"markdown","metadata":{"id":"788v7Vk-QYif"},"source":["<font color='green'>Fin Actividad 2</font>"]},{"cell_type":"markdown","metadata":{"id":"AMWz9_nJJ_zI"},"source":["<img src=\"https://drive.google.com/uc?export=view&id=1UT8QmOplTfNn0iH3FTtAQhXCHzS0M6si\" width=\"100\" align=\"left\" title=\"Runa-perth\">\n","<br clear=\"left\">\n","\n","## <font color='black'>**Contenido opcional**</font>\n","\n","## <font color='blue'>**ICA**</font>\n","\n","<img src='https://drive.google.com/uc?export=view&id=1X81nmlZUhroOJfgU8vbVLGzkEy2ukcDr' width=\"500\" align=\"centered\" >\n","<br/>\n","\n","Análisis de Componentes Independientes, o ICA para abreviar. ICA es un método computacional para separar una señal multivariante en sus componentes subyacentes. Con ICA, podemos extraer el componente deseado de la fusión de múltiples señales.\n","\n","\n","## Precondiciones\n","\n","La primera condición previa para que funcione el algoritmo es que las señales mixtas sean una combinación lineal de cualquier número de señales fuente.\n","\n","La segunda condición previa es que las señales fuente sean independientes. Entonces, ¿qué significa independencia? Dos señales son independientes si la información de la señal s1 no proporciona ninguna información sobre la señal s2. Esto implica que no están correlacionados, lo que significa que su covarianza es 0. Sin embargo, hay que tener cuidado aquí, ya que la falta de correlación no significa automáticamente independencia.\n","\n","La tercera condición previa es que los componentes independientes no sean gaussianos. ¿Porqué es eso? La distribución de densidad conjunta de dos señales independientes no gaussianas será uniforme en un cuadrado;\n","\n","\n","En un nivel general, ICA se puede dividir en los siguientes pasos.\n","1. Centro x restando la media\n","2. *Whitening* x\n","3. Elija un valor inicial aleatorio para la matriz de desmezcla w\n","4. Calcule el nuevo valor de w\n","5. Normalizar w\n","6. Compruebe si el algoritmo ha convergido y, si no, vuelva al paso 4\n","7. Tome el producto escalar de w y x para obtener las señales de fuente independientes\n","\n","Que significa *Whitening*\n","\n","*Whitening* una señal dada significa que la transformamos de tal manera que se eliminan las correlaciones potenciales entre sus componentes (covarianza igual a 0) y la varianza de cada componente es igual a 1. Otra forma de verlo es que el La matriz de covarianza de la señal *Whitening* será igual a la matriz de identidad.\n","\n","\n","\n","```\n","def whitening(X):\n","    cov = np.cov(X) # Matria de covarianza\n","    d, E = np.linalg.eigh(cov) # Autovalores y Autovectores\n","    D = np.diag(d) # Matriz de 3x3 con los valores de los autovalores\n","    D_inv = np.sqrt(np.linalg.inv(D)) # Invertimos para que valgan 1.\n","    X_whiten = np.dot(E, np.dot(D_inv, np.dot(E.T, X)))\n","    return X_whiten\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qf0Pkc7LO3Kh"},"source":["¿Cómo funciona? Como se discutió anteriormente, una condición previa para que ICA funcione es que nuestras señales de origen no sean gaussianas. Una cosa interesante acerca de dos señales independientes no gaussianas es que su suma es más gaussiana que cualquiera de las señales fuente. Por lo tanto, necesitamos optimizar W de manera que las señales resultantes de Wx sean lo más no gaussianas posible. Para hacerlo, necesitamos una medida de gaussianidad. La medida más simple sería la curtosis, que es el cuarto momento de los datos y mide la \"cola\" de una distribución. Sin embargo para una mejor convergencia se utiliza\n","\n","*Tichavský, P., Koldovský, Z., & Oja, E. (2007, September). Speed and accuracy enhancement of linear ICA techniques using rational nonlinear functions. In International Conference on Independent Component Analysis and Signal Separation (pp. 285-292). Springer, Berlin, Heidelberg.*\n","\n","\n","$$g(\\mu) = \\tanh(\\mu)$$ junto con su derivada\n","$$g'(\\mu) = 1 - \\tanh^2(\\mu)$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FyJcCCeje986"},"source":["## <font color='green'>Actividad 3</font>\n","\n","Considere las siguientes señales:\n","\n","\n","\n","```\n","n_samples = 2000\n","time = np.linspace(0, 8, n_samples)\n","s1 = np.sin(2 * time)\n","s2 = np.sign(np.sin(3 * time)) # square signal\n","s3 = signal.sawtooth(2 * np.pi * time)\n","```\n","\n","1. Grafique las señales originales\n","\n","2. Combine las señales como por ejemplo :\n","\n","\n","```\n","X = np.c_[s1, s2, s3]\n","A = np.array(([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]]))\n","X = np.dot(X, A.T)\n","```\n","4. Grafique las señales mezcladas\n","5. Aplique FastICA y vuelva a graficar.\n","6. Compare con PCA.\n","\n"]},{"cell_type":"code","metadata":{"id":"aA4fheX0QqAn"},"source":["import numpy as np\n","np.random.seed(0)\n","from scipy import signal\n","from scipy.io import wavfile\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","from sklearn.decomposition import FastICA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPYakQVyZtW8"},"source":["def plot_mixture_sources_predictions(X, original_sources, S):\n","    fig = plt.figure()\n","    plt.subplot(3, 1, 1)\n","    for x in X:\n","        plt.plot(x)\n","    plt.title(\"mixtures\")\n","    plt.subplot(3, 1, 2)\n","    for s in original_sources:\n","        plt.plot(s)\n","    plt.title(\"real sources\")\n","    plt.subplot(3,1,3)\n","    for s in S:\n","        plt.plot(s)\n","    plt.title(\"predicted sources\")\n","\n","    fig.tight_layout()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lhiSITTPQkhL"},"source":["n_samples = 2000\n","time = np.linspace(0, 8, n_samples)\n","s1 = np.sin(2 * time)\n","s2 = np.sign(np.sin(3 * time)) # square signal\n","s3 = signal.sawtooth(2 * np.pi * time)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2YSkZeAafDI6"},"source":["<font color='green'>Fin Actividad 3</font>\n","\n","<img src=\"https://drive.google.com/uc?export=view&\n","id=1UT8QmOplTfNn0iH3FTtAQhXCHzS0M6si\" width=\"50\" align=\"left\" title=\"Runa-perth\">"]},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?export=view&id=1bqkJJ7QiIOTsp-7jm5eUtK-XYWMEq2_K\" width=\"100\" align=\"left\" />\n","<br clear=\"left\">\n","\n","## <font color='blue'>**Resumen**</font>\n","\n","Resumen de t-SNE (t-distributed Stochastic Neighbor Embedding):\n","\n","t-SNE es una técnica de reducción de dimensionalidad no lineal utilizada principalmente para visualizar datos de alta dimensionalidad en un espacio bidimensional o tridimensional. Su objetivo es preservar la estructura local de los datos, manteniendo las distancias entre puntos similares y disminuyendo las distancias entre puntos no similares.\n","\n","Funcionamiento:\n","\n","1. Medición de similitudes: t-SNE calcula las similitudes entre pares de puntos en el espacio de alta dimensión utilizando una distribución de probabilidad basada en la distancia euclidiana.\n","\n","2. Reducción de dimensionalidad: Luego, t-SNE mapea los datos a un espacio de baja dimensión, como 2D o 3D, de manera que la similitud de los puntos se conserve en la medida de lo posible.\n","\n","Ventajas de t-SNE:\n","\n","1. Preservación de estructura local: t-SNE es efectivo para preservar la estructura local y agrupaciones de datos en un espacio de baja dimensión.\n","\n","2. Visualización de datos complejos: Es útil para visualizar datos complejos en 2D o 3D para la identificación de patrones y relaciones.\n","\n","Desventajas de t-SNE:\n","\n","1. Costo computacional: t-SNE puede ser computacionalmente costoso para grandes conjuntos de datos debido a la alta dimensionalidad y la necesidad de calcular las similitudes de cada par de puntos.\n","\n","2. No siempre preserva la estructura global: t-SNE puede tener dificultades para preservar la estructura global de los datos y puede comprimir regiones densas de puntos.\n","\n","Ejemplos de aplicaciones de t-SNE:\n","\n","1. Visualización de datos en aprendizaje automático: t-SNE es ampliamente utilizado para visualizar características y embeddings en tareas de clasificación y agrupamiento.\n","\n","2. Visualización de datos biológicos: En bioinformática, t-SNE se aplica para visualizar datos de expresión genética y agrupar células en estudios de células individuales.\n","\n","3. Procesamiento de lenguaje natural: En el análisis de texto, t-SNE puede visualizar embeddings de palabras o documentos para entender relaciones semánticas.\n","\n","Resumen de Isomap:\n","\n","Isomap es una técnica de reducción de dimensionalidad no lineal que se basa en la estructura geométrica de los datos. Su objetivo es preservar las distancias geodésicas (distancias a lo largo de las curvas más cortas en una variedad) entre puntos en un espacio de baja dimensión.\n","\n","Funcionamiento:\n","\n","1. Cálculo de vecindades: Isomap encuentra los k vecinos más cercanos de cada punto en el espacio de alta dimensión.\n","\n","2. Construcción del grafo: Luego, construye un grafo de vecinos conectando puntos con aristas ponderadas según las distancias euclidianas entre ellos.\n","\n","3. Cálculo de distancias geodésicas: Isomap estima las distancias geodésicas entre todos los pares de puntos utilizando el algoritmo de Dijkstra.\n","\n","4. Reducción de dimensionalidad: Finalmente, mapea los datos a un espacio de baja dimensión utilizando técnicas de análisis de componentes principales.\n","\n","Ventajas de Isomap:\n","\n","1. Preservación de estructura global: Isomap es efectivo para preservar la estructura global y la conectividad de los datos en un espacio de baja dimensión.\n","\n","2. Robustez frente a datos ruidosos: Es menos sensible al ruido y a valores atípicos en comparación con otras técnicas de reducción de dimensionalidad.\n","\n","Desventajas de Isomap:\n","\n","1. Costo computacional: Al igual que t-SNE, Isomap puede ser computacionalmente costoso para grandes conjuntos de datos debido al cálculo de las distancias geodésicas.\n","\n","Ejemplos de aplicaciones de Isomap:\n","\n","1. Visualización y análisis de datos de sensores: Isomap es útil para visualizar y entender datos de sensores multidimensionales en entornos IoT y sistemas embebidos.\n","\n","2. Procesamiento de imágenes: En visión por computadora, Isomap puede ser utilizado para reducir la dimensionalidad de características de imágenes para tareas de reconocimiento y clasificación.\n","\n","3. Análisis de datos geoespaciales: En geoinformática, Isomap se aplica para visualizar y analizar datos de ubicaciones geográficas en espacios de baja dimensión.\n","\n","En resumen, t-SNE y Isomap son técnicas de reducción de dimensionalidad no lineales con el objetivo de preservar la estructura y la conectividad de los datos en un espacio de baja dimensión. Ambos son útiles para visualizar datos complejos y entender las relaciones entre puntos en conjuntos de datos de alta dimensionalidad. Sin embargo, es importante seleccionar la técnica adecuada según los requisitos específicos de la aplicación y considerar el costo computacional para conjuntos de datos grandes."],"metadata":{"id":"5Udia-cvlbyY"}}]}